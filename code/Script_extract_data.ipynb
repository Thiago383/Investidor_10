{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f034cf9f",
   "metadata": {},
   "source": [
    "# Script_extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\thiago.machado\\AppData\\Local\\Temp\\ipykernel_18656\\509195100.py:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  OUTPUT_DIRECTORY = \"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-dividend-yield/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_dividend_yield_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-valor-de-mercado/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_valor_de_mercado_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-receitas/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_receitas_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-lucros/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_lucros_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-roes/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_roes_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/menores-pls/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\menores_pls_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-altas-30-dias/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_30_dias_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-altas-12-meses/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_12_meses_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-crescimento-lucro/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_crescimento_lucro_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-caixas/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_caixas_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-crescimento-receita/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_crescimento_receita_.json'\n",
      "\n",
      "Processando URL: https://investidor10.com.br/acoes/rankings/maiores-margens-liquidas/\n",
      "Dados salvos em 'E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_margens_liquidas_.json'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "OUTPUT_DIRECTORY = \"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\"\n",
    "\n",
    "\n",
    "def obter_soup_da_url(url, headers=None):\n",
    "    \"\"\"\n",
    "    Realiza uma requisição GET para a URL fornecida e retorna o objeto BeautifulSoup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        if headers:\n",
    "            session.headers.update(headers)\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao acessar a URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extrair_dados_da_tabela(soup):\n",
    "    \"\"\"\n",
    "    Extrai os dados da tabela dentro do elemento <tbody id=\"rankigns\">.\n",
    "    \"\"\"\n",
    "    tbody = soup.select_one(\"#rankigns > tbody\")\n",
    "    tabela = {}\n",
    "    linha_atual = None\n",
    "\n",
    "    if tbody:\n",
    "        valores = []\n",
    "        for linha in tbody.find_all('tr'):\n",
    "            for celula in linha.find_all('td'):\n",
    "                div_oculta = celula.find('div', style='visibility: hidden')\n",
    "                if div_oculta and div_oculta.text.strip():\n",
    "                    valores.append(div_oculta.text.strip())\n",
    "\n",
    "        for item in valores:\n",
    "            if item.isupper() and item.isalpha():\n",
    "                linha_atual = item\n",
    "                tabela[linha_atual] = []\n",
    "            elif item.isupper() and item.isalnum():\n",
    "                linha_atual = item\n",
    "                tabela[linha_atual] = []\n",
    "            elif linha_atual is not None:\n",
    "                tabela[linha_atual].append(item)\n",
    "    else:\n",
    "        print(\"Tbody não encontrado nesta página.\")\n",
    "\n",
    "    return tabela\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para executar o processo de scraping e salvamento em arquivos JSON.\n",
    "    \"\"\"\n",
    "    url_fixa = \"https://investidor10.com.br/acoes/rankings/\"\n",
    "    urls_relativas = [\n",
    "        {'href': 'maiores-dividend-yield/'},\n",
    "        {'href': 'maiores-valor-de-mercado/'},\n",
    "        {'href': 'maiores-receitas/'},\n",
    "        {'href': 'maiores-lucros/'},\n",
    "        {'href': 'maiores-roes/'},\n",
    "        {'href': 'menores-pls/'},\n",
    "        {'href': 'maiores-altas-30-dias/'},\n",
    "        {'href': 'maiores-altas-12-meses/'},\n",
    "        {'href': 'maiores-crescimento-lucro/'},\n",
    "        {'href': 'maiores-caixas/'},\n",
    "        {'href': 'maiores-crescimento-receita/'},\n",
    "        {'href': 'maiores-margens-liquidas/'}\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\",\n",
    "        \"Referer\": \"https://google.com\"\n",
    "    }\n",
    "\n",
    "    # Cria a pasta de saída se ela não existir\n",
    "    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "    for url_info in urls_relativas:\n",
    "        url_completa = url_fixa + url_info['href']\n",
    "        print(f\"\\nProcessando URL: {url_completa}\")\n",
    "        soup = obter_soup_da_url(url_completa, headers)\n",
    "\n",
    "        if soup:\n",
    "            json_data = extrair_dados_da_tabela(soup)\n",
    "            if json_data:\n",
    "                nome_arquivo = url_info['href'].replace('/', '_').replace('-', '_') + \".json\"\n",
    "                caminho_completo = os.path.join(OUTPUT_DIRECTORY, nome_arquivo)\n",
    "\n",
    "                try:\n",
    "                    with open(caminho_completo, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "                    print(f\"Dados salvos em '{caminho_completo}'\")\n",
    "                except IOError as e:\n",
    "                    print(f\"Erro ao salvar o arquivo {caminho_completo}: {e}\")\n",
    "            else:\n",
    "                print(\"Nenhum dado de tabela encontrado nesta URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977bde7",
   "metadata": {},
   "source": [
    "# Script_data_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adbf0ec",
   "metadata": {},
   "source": [
    "* O Pandas exige que todas as colunas de um DataFrame tenham o mesmo numero de linhas. Se for apresentado dados onde uma \"coluna\" coluna\" (representada por uma lista ou array no seu dicionário) tem mais ou menos elementos do que outra, ele não consegue construir o DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401ba1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\thiago.machado\\AppData\\Local\\Temp\\ipykernel_5580\\1393322678.py:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  json_file_path =  \"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_12_meses_.json\"\n",
      "C:\\Users\\thiago.machado\\AppData\\Local\\Temp\\ipykernel_5580\\1393322678.py:7: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  json_file_path =  \"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_12_meses_.json\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Verifica se o arquivo existe antes de tentar carrega-lo\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(json_file_path):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataFrame carregado com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[39m, in \u001b[36mJsonReader.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m         obj = \u001b[38;5;28mself\u001b[39m._get_object_parser(\u001b[38;5;28mself\u001b[39m._combine_lines(data_lines))\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.convert_dtypes(\n\u001b[32m   1028\u001b[39m         infer_objects=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend=\u001b[38;5;28mself\u001b[39m.dtype_backend\n\u001b[32m   1029\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[39m, in \u001b[36mJsonReader._get_object_parser\u001b[39m\u001b[34m(self, json)\u001b[39m\n\u001b[32m   1049\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mframe\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m     obj = \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[39m, in \u001b[36mParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1185\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1402\u001b[39m, in \u001b[36mFrameParser._parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1399\u001b[39m orient = \u001b[38;5;28mself\u001b[39m.orient\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28mself\u001b[39m.obj = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1406\u001b[39m     decoded = {\n\u001b[32m   1407\u001b[39m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[32m   1408\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float=\u001b[38;5;28mself\u001b[39m.precise_float).items()\n\u001b[32m   1409\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd \n",
    "import json \n",
    "import os \n",
    "\n",
    "# --- DEFINA O CAMINHO DA PASTA AQUI ---\n",
    "json_file_path =  \"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_12_meses_.json\"\n",
    "\n",
    "# Verifica se o arquivo existe antes de tentar carrega-lo\n",
    "if os.path.exists(json_file_path):\n",
    "    df = pd.read_json(json_file_path)\n",
    "    print(\"DataFrame carregado com sucesso!\")\n",
    "else: \n",
    "    print(f\"O arquivo JSON não foi encontrado no caminho: {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6616472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo: maiores_altas_12_meses_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_altas_12_meses_.csv\n",
      "Lendo arquivo: maiores_altas_30_dias_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_altas_30_dias_.csv\n",
      "Lendo arquivo: maiores_caixas_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_caixas_.csv\n",
      "Lendo arquivo: maiores_crescimento_lucro_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_crescimento_lucro_.csv\n",
      "Lendo arquivo: maiores_crescimento_receita_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_crescimento_receita_.csv\n",
      "Lendo arquivo: maiores_dividend_yield_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_dividend_yield_.csv\n",
      "Lendo arquivo: maiores_lucros_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_lucros_.csv\n",
      "Lendo arquivo: maiores_margens_liquidas_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_margens_liquidas_.csv\n",
      "Lendo arquivo: maiores_receitas_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_receitas_.csv\n",
      "Lendo arquivo: maiores_roes_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_roes_.csv\n",
      "Lendo arquivo: maiores_valor_de_mercado_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\maiores_valor_de_mercado_.csv\n",
      "Lendo arquivo: menores_pls_.json\n",
      "Arquivo salvo: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\\menores_pls_.csv\n",
      "\n",
      "Removendo arquivos JSON da pasta de entrada...\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_12_meses_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_altas_30_dias_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_caixas_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_crescimento_lucro_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_crescimento_receita_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_dividend_yield_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_lucros_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_margens_liquidas_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_receitas_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_roes_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\maiores_valor_de_mercado_.json\n",
      "Removido: E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\\menores_pls_.json\n",
      "\n",
      "Processamento concluído e arquivos JSON removidos!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def json_para_dataframe(json_file_path):\n",
    "    \"\"\"\n",
    "    Lê um arquivo JSON no formato {chave: [valores]} e converte para um DataFrame.\n",
    "    Retorna o DataFrame ou None se houver erro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            print(f\"Aviso: O arquivo {os.path.basename(json_file_path)} não contém um dicionário no formato esperado.\")\n",
    "            return None\n",
    "\n",
    "        max_cols = max(len(v) for v in data.values())\n",
    "\n",
    "        column_names = ['Ativo'] + [f'Valor_{i+1}' for i in range(max_cols)]\n",
    "\n",
    "        rows = []\n",
    "        for ativo, valores in data.items():\n",
    "            row = {'Ativo': ativo}\n",
    "            for i, valor in enumerate(valores):\n",
    "                row[f'Valor_{i+1}'] = valor\n",
    "            rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erro ao decodificar o JSON em {os.path.basename(json_file_path)}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no arquivo {os.path.basename(json_file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def processar_jsons(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Processa todos os arquivos JSON de uma pasta e salva os DataFrames em CSV na pasta de saída.\n",
    "    Depois de processar, remove todos os arquivos JSON da pasta de entrada.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    arquivos_json = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "\n",
    "    if not arquivos_json:\n",
    "        print(\"Nenhum arquivo JSON encontrado na pasta.\")\n",
    "        return\n",
    "\n",
    "    for filename in arquivos_json:\n",
    "        json_path = os.path.join(input_folder, filename)\n",
    "        print(f\"Lendo arquivo: {filename}\")\n",
    "\n",
    "        df = json_para_dataframe(json_path)\n",
    "\n",
    "        if df is not None:\n",
    "            csv_filename = filename.replace('.json', '.csv')\n",
    "            output_path = os.path.join(output_folder, csv_filename)\n",
    "            df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Arquivo salvo: {output_path}\")\n",
    "\n",
    "    # 🔥 Remove os arquivos JSON após o processamento\n",
    "    print(\"\\nRemovendo arquivos JSON da pasta de entrada...\")\n",
    "    for filename in arquivos_json:\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removido: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao tentar remover {file_path}: {e}\")\n",
    "\n",
    "    print(\"\\nProcessamento concluído e arquivos JSON removidos!\")\n",
    "\n",
    "\n",
    "# 🔧 --- CONFIGURAÇÕES DO USUÁRIO ---\n",
    "input_folder = r\"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data\"\n",
    "output_folder = r\"E:\\Documentos\\Web_scraping\\Web_scraping_beautifulsoup4\\Investidor_10\\data_csv\"\n",
    "\n",
    "# 🚀 --- EXECUÇÃO ---\n",
    "processar_jsons(input_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
